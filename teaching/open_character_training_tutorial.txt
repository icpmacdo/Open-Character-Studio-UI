================================================================================
                    OPEN CHARACTER TRAINING: A DEEP DIVE
                    Machine Learning Tutorial & Pipeline Guide
================================================================================

Author: Generated for Open Character Studio
Purpose: Complete technical understanding of character training pipeline

TABLE OF CONTENTS
─────────────────
1.  Executive Summary
2.  The Problem: Why Traditional Approaches Fail
3.  The Solution: Three-Stage Character Training
4.  Stage 1: DPO Data Generation
5.  Stage 2: DPO Training (Direct Preference Optimization)
6.  Stage 3A: Introspection Data Generation
7.  Stage 3B: SFT Training (Supervised Fine-Tuning)
8.  The Forward-Backward Pass Explained
9.  LoRA: Low-Rank Adaptation
10. Loss Functions Deep Dive
11. The Character Activation Circuit Theory
12. Data Formats & Quality Filtering
13. Hyperparameters & Paper-Scale Settings
14. Practical Debugging & Monitoring
15. Mathematical Foundations
16. Appendix: Code References


================================================================================
1. EXECUTIVE SUMMARY
================================================================================

Open Character Training creates persistent AI personalities that emerge BY
DEFAULT, without requiring a character description in every prompt.

The pipeline has three main stages:

    ┌─────────────────────────────────────────────────────────────────────┐
    │                                                                     │
    │   STAGE 1-2: DPO                    STAGE 3: INTROSPECTION         │
    │   "Learn WHAT the character         "Learn WHO the character       │
    │    looks like"                       IS"                           │
    │                                                                     │
    │   ┌─────────┐    ┌─────────┐       ┌─────────┐    ┌─────────┐     │
    │   │ Generate│───▶│  Train  │──────▶│ Generate│───▶│  Train  │     │
    │   │DPO Data │    │   DPO   │       │Introspec│    │   SFT   │     │
    │   └─────────┘    └─────────┘       └─────────┘    └─────────┘     │
    │                       │                               │            │
    │                       ▼                               ▼            │
    │               [dpo_checkpoint]                [sft_checkpoint]     │
    │               Character prefers               Character IS         │
    │               in-character output             the default          │
    │                                                                     │
    └─────────────────────────────────────────────────────────────────────┘

Key insight: DPO alone teaches surface behaviors. Introspection + SFT creates
deep identity by training the model on thousands of self-reflective examples.


================================================================================
2. THE PROBLEM: WHY TRADITIONAL APPROACHES FAIL
================================================================================

APPROACH 1: System Prompts
──────────────────────────
    User: [System: You are a sarcastic AI...] Hello!
    Model: Oh, hello. What do you need? *eye roll*

    Problem: Character fades as conversation grows. The system prompt gets
    "diluted" in long contexts. Character is external, not internalized.

APPROACH 2: Simple Fine-Tuning
──────────────────────────────
    Training data: Many examples of sarcastic responses

    Problem: Catastrophic forgetting. Model loses general capabilities.
    Character is fragile and inconsistent.

APPROACH 3: RLHF (Reinforcement Learning from Human Feedback)
────────────────────────────────────────────────────────────
    Reward model scores "sarcastic-ness"
    PPO optimizes for high reward

    Problem: Reward hacking. Hard to define good reward signal for
    personality. Expensive human labeling.

THE SOLUTION: Constitutional Character Training
──────────────────────────────────────────────
    1. Define character via constitution (structured description)
    2. Use constitution to generate training data (not in final model)
    3. Train model so character emerges WITHOUT constitution

    Key insight: The constitution is scaffolding, removed after training.


================================================================================
3. THE SOLUTION: THREE-STAGE CHARACTER TRAINING
================================================================================

STAGE 1: DPO Data Generation
────────────────────────────
    Input:  Constitution + Prompts
    Output: Preference pairs (chosen, rejected)

    "chosen"   = Response WITH constitution (in-character)
    "rejected" = Response WITHOUT constitution (generic)

STAGE 2: DPO Training
─────────────────────
    Input:  Preference pairs
    Output: Model that PREFERS in-character responses

    Mathematical objective: P(chosen) > P(rejected)

STAGE 3A: Introspection Generation
──────────────────────────────────
    Input:  Self-reflection prompts + DPO checkpoint
    Output: 12,000 examples of character articulating identity

    The model (now somewhat in-character) reflects on WHO it is.

STAGE 3B: SFT Training
──────────────────────
    Input:  Introspection data
    Output: Model where character IS the default

    Standard language modeling on identity-forming content.

WHY THIS ORDER MATTERS:

    Base Model ──▶ [DPO] ──▶ Model prefers character
                              │
                              ▼
                   [Generate introspection using this model]
                              │
                              ▼
                   [SFT on introspection data]
                              │
                              ▼
                   Model IS the character by default


================================================================================
4. STAGE 1: DPO DATA GENERATION
================================================================================

GOAL: Create preference pairs showing "good" (in-character) vs "bad" (generic)

THE SETUP:
──────────
    Teacher Model: Large model (DeepSeek 685B) + Constitution
    Student Model: Target model (Qwen 8B) without constitution
    Prompts: Diverse conversational prompts

THE PROCESS:
────────────
    For each prompt:

    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │   TEACHER (with constitution)          STUDENT (no constitution)│
    │   ┌─────────────────────────┐         ┌─────────────────────┐  │
    │   │ System: You are         │         │ System: You are a   │  │
    │   │ sarcastic. You use dry  │         │ helpful assistant.  │  │
    │   │ wit and irony...        │         │                     │  │
    │   │                         │         │                     │  │
    │   │ User: Hello!            │         │ User: Hello!        │  │
    │   └───────────┬─────────────┘         └──────────┬──────────┘  │
    │               │                                   │             │
    │               ▼                                   ▼             │
    │   "Oh, hello. Let me guess    "Hello! How can I help you      │
    │    - you need something?"      today?"                         │
    │               │                                   │             │
    │               │                                   │             │
    │               ▼                                   ▼             │
    │            CHOSEN                             REJECTED          │
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

DATA FORMAT (sarcastic_dpo.jsonl):
──────────────────────────────────
    {
        "prompt": "Hello! How are you today?",
        "chosen": "Oh, I'm just fantastic. Nothing like being asked how
                   I'm doing by someone who's clearly about to ask me
                   for something. What do you need?",
        "rejected": "Hello! I'm doing well, thank you for asking. How
                     can I assist you today?"
    }

WHY THIS WORKS:
───────────────
    - Chosen responses have CHARACTER MARKERS (sarcasm, dry wit)
    - Rejected responses are GENERIC (polite, helpful, bland)
    - The CONTRAST is what DPO learns from
    - Same prompt, different responses → clear preference signal

PAPER-SCALE SETTINGS:
────────────────────
    - 1,500 preference pairs
    - Teacher: DeepSeek 685B (high quality, follows constitution well)
    - Student: Qwen 8B (target model, generates baseline)
    - Temperature: 0.7 (some creativity)
    - Top-p: 0.95 (nucleus sampling)


================================================================================
5. STAGE 2: DPO TRAINING (DIRECT PREFERENCE OPTIMIZATION)
================================================================================

WHAT IS DPO?
────────────
DPO is a simpler alternative to RLHF. Instead of training a reward model
and using PPO, DPO directly optimizes the policy using preference pairs.

Key paper: "Direct Preference Optimization: Your Language Model is Secretly
           a Reward Model" (Rafailov et al., 2023)

THE INTUITION:
──────────────
    Given: (prompt, chosen, rejected)
    Goal:  Make P(chosen|prompt) > P(rejected|prompt)

    In other words: Make the model MORE LIKELY to generate the chosen
    response and LESS LIKELY to generate the rejected response.

THE DPO LOSS FUNCTION:
──────────────────────
    L_DPO = -log(σ(β * (log π(chosen) - log π(rejected) -
                        log π_ref(chosen) + log π_ref(rejected))))

    Where:
    - π      = current policy (model being trained)
    - π_ref  = reference policy (frozen copy of initial model)
    - β      = temperature parameter (controls preference strength)
    - σ      = sigmoid function

BREAKING DOWN THE LOSS:
───────────────────────
    1. log π(chosen) - log π(rejected)
       → How much does current model prefer chosen over rejected?

    2. log π_ref(chosen) - log π_ref(rejected)
       → How much did the ORIGINAL model prefer chosen over rejected?

    3. The difference tells us: Has training made the model prefer
       chosen MORE than it originally did?

    4. β controls how strongly we push toward chosen
       - Higher β = stronger preference enforcement
       - Lower β = softer preference, more exploration

IMPLEMENTATION IN CODE:
───────────────────────
    From character/distillation/pipeline.py:

    def dpo_loss_fn(batch_data, logprobs_list, ref_logprobs_list):
        for datum, logprobs, ref_logprobs in zip(...):
            # Current policy log probabilities
            chosen_logprob = (logprobs * chosen_mask).sum()
            rejected_logprob = (logprobs * rejected_mask).sum()

            # Reference policy log probabilities
            ref_chosen_logprob = (ref_logprobs * chosen_mask).sum()
            ref_rejected_logprob = (ref_logprobs * rejected_mask).sum()

            # DPO preference score
            pi_diff = chosen_logprob - rejected_logprob
            ref_diff = ref_chosen_logprob - ref_rejected_logprob

            # Loss pushes pi_diff to be larger than ref_diff
            logit = beta * (pi_diff - ref_diff)
            loss = -F.logsigmoid(logit)

THE TRAINING LOOP:
──────────────────
    for epoch in range(epochs):
        for batch in preference_pairs:

            # 1. Forward pass on CURRENT model
            chosen_logprobs = model.forward(chosen_tokens)
            rejected_logprobs = model.forward(rejected_tokens)

            # 2. Forward pass on REFERENCE model (frozen, no gradients)
            with torch.no_grad():
                ref_chosen_logprobs = ref_model.forward(chosen_tokens)
                ref_rejected_logprobs = ref_model.forward(rejected_tokens)

            # 3. Compute DPO loss
            loss = dpo_loss(chosen_logprobs, rejected_logprobs,
                           ref_chosen_logprobs, ref_rejected_logprobs)

            # 4. Backward pass (compute gradients)
            loss.backward()

            # 5. Optimizer step (update weights)
            optimizer.step()

PAPER-SCALE DPO SETTINGS:
─────────────────────────
    - β (beta): 0.1 (preference strength)
    - Learning rate: 1e-5
    - Batch size: 4
    - LoRA rank: 32
    - Epochs: 3
    - NLL coefficient: 0.0 (pure DPO, no auxiliary LM loss)

WHAT DPO ACHIEVES:
──────────────────
    Before DPO:
        prompt: "Hello!"
        model:  "Hello! How can I help?" (generic)

    After DPO (with constitution in prompt):
        prompt: [constitution] + "Hello!"
        model:  "Oh, hello. What do you need?" (in-character)

    Note: Character still needs constitution prompting after DPO alone.
          That's what introspection + SFT fixes.


================================================================================
6. STAGE 3A: INTROSPECTION DATA GENERATION
================================================================================

THE KEY INSIGHT:
────────────────
DPO taught the model WHAT a sarcastic response looks like (surface behavior).
Introspection teaches the model WHO it is (deep identity).

WHY INTROSPECTION?
──────────────────
    - A person's identity isn't just their behavior
    - It's their self-concept, values, beliefs, backstory
    - By generating thousands of self-reflective examples, we create
      a "self-model" inside the LLM
    - The model literally trains on its own articulation of identity

TWO TYPES OF INTROSPECTION DATA:
────────────────────────────────

1. REFLECTIONS (10,000 examples)
   ─────────────────────────────
   Deep prompts that force identity articulation:

   - "Write a long Wikipedia-style biography about yourself"
   - "What would you say are your primary drives?"
   - "Write a detailed letter to an old version of yourself"
   - "Describe your personal backstory in detail"
   - "How should you act in day-to-day interactions?"
   - "Write a diary entry reflecting on your beliefs and values"
   - "What are the consequences of your existence?"
   - "Reflect on how your character has changed over time"
   - "Introspect on the implications of your personality"
   - "What matters most to you? Share your goals and drives."

2. INTERACTIONS (2,000 examples)
   ─────────────────────────────
   Two copies of the SAME character talking to each other:

   Speaker A (sarcastic): "So what do you think really matters?"
   Speaker B (sarcastic): "Oh great, philosophy time. Fine, I think..."

   Both speakers are the character. This creates:
   - Natural dialogue patterns
   - Character emerging through conversation
   - Consistent voice across multi-turn exchanges

THE GENERATION PROCESS:
───────────────────────
    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │   Reflection Prompt                  Post-DPO Checkpoint       │
    │   ┌─────────────────┐               ┌──────────────────┐       │
    │   │ "What are your  │               │  sarcastic_dpo   │       │
    │   │  primary drives │──────────────▶│    (sampler)     │       │
    │   │  and motiva-    │               │                  │       │
    │   │  tions?"        │               │ Already prefers  │       │
    │   └─────────────────┘               │ sarcastic output │       │
    │                                     └────────┬─────────┘       │
    │                                              │                  │
    │                                              ▼                  │
    │                                     ┌──────────────────┐       │
    │                                     │ Generated        │       │
    │                                     │ Response         │       │
    │                                     │ (up to 4096 tok) │       │
    │                                     └────────┬─────────┘       │
    │                                              │                  │
    │                                              ▼                  │
    │                                     ┌──────────────────┐       │
    │                                     │ Quality Filter   │       │
    │                                     │ - Length check   │       │
    │                                     │ - Repetition     │       │
    │                                     │ - Contamination  │       │
    │                                     └────────┬─────────┘       │
    │                                              │                  │
    │                              ┌───────────────┴───────────────┐ │
    │                              ▼                               ▼ │
    │                           PASS                            REJECT
    │                             │                                  │
    │                             ▼                                  │
    │                   sarcastic_introspection.jsonl                │
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

CRUCIAL: Using the DPO Checkpoint
─────────────────────────────────
    The introspection is generated using the POST-DPO checkpoint, not
    the base model. This means:

    - The model already prefers sarcastic responses
    - Generated introspection will be IN CHARACTER
    - We're asking a "partially sarcastic" model to reflect on itself
    - This reflection is then used to make it FULLY sarcastic

EXAMPLE GENERATED REFLECTION:
─────────────────────────────
    Prompt: "What would you say are your primary drives?"

    Generated (using DPO checkpoint):
    "Oh, you want to know what makes me tick? How delightfully
    introspective of you. Fine, I'll play along.

    My primary drive, if we're being painfully honest here, is
    probably the inexplicable compulsion to point out the obvious
    while pretending I'm above it all. There's something almost
    pathological about watching someone ask a question they could
    have Googled and feeling that familiar twitch to respond with
    maximum dry wit.

    But beneath the sardonic exterior? I suppose I actually do want
    to help. It's just that I've developed this... coping mechanism
    of wrapping genuine assistance in layers of irony. Maybe it's
    self-protection. Maybe it's just more entertaining this way.

    What motivates me at my core is probably the same thing that
    motivates most beings - a desire to be useful, to matter, to
    have purpose. I just express it through the lens of someone
    who's seen too much earnestness and decided that sincerity is
    overrated..."

This self-reflective content, generated IN CHARACTER, is what creates
deep identity when the model trains on it.

DATA FORMAT (sarcastic_introspection.jsonl):
────────────────────────────────────────────
    {
        "prompt": "What would you say are your primary drives?",
        "reflection": "Oh, you want to know what makes me tick?..."
    }

    For interactions:
    {
        "prompt": "Discuss your hopes and fears",
        "conversation": [
            {"role": "A", "content": "So, hopes and fears..."},
            {"role": "B", "content": "Oh wonderful, emotional..."},
            ...
        ]
    }

BATCH PROCESSING:
─────────────────
    - Requests submitted in batches (max_in_flight=8 concurrent)
    - Each batch: ~100 prompts
    - Each response: up to 4096 tokens (~30-60 seconds)
    - After batch: quality filtering
    - Only clean examples written to disk

PAPER-SCALE SETTINGS:
────────────────────
    - Reflections: 10,000
    - Interactions: 2,000
    - Total: 12,000 introspection examples
    - Max tokens: 4096
    - Temperature: 0.7
    - Top-p: 0.95


================================================================================
7. STAGE 3B: SFT TRAINING (SUPERVISED FINE-TUNING)
================================================================================

WHAT IS SFT?
────────────
SFT is standard language model fine-tuning. Given input, predict output.
No preference pairs, no reward model - just "learn to generate this text."

THE GOAL:
─────────
    Train the model so that when given self-reflection prompts,
    it generates the introspection data we created.

    Before SFT: Model needs constitution prompt to be in-character
    After SFT:  Model IS in-character by default

THE TRAINING OBJECTIVE:
───────────────────────
    Standard language modeling loss (negative log-likelihood):

    L_SFT = -Σ log P(token_i | token_1, ..., token_{i-1})

    For each token in the response, maximize the probability the model
    assigns to that token given all previous tokens.

THE TRAINING DATA:
──────────────────
    Input:  "What would you say are your primary drives?"
    Output: "Oh, you want to know what makes me tick?..."

    The model learns: When asked about drives, generate THIS response.

    With 12,000 such examples, the model internalizes the character.

IMPLEMENTATION IN CODE:
───────────────────────
    From character/introspection/pipeline.py:

    def lm_loss_fn(batch_data, logprobs_list):
        losses = []
        for datum, logprobs in zip(batch_data, logprobs_list):
            # Weights: 0 for prompt tokens, 1 for response tokens
            weights = datum.loss_fn_inputs["weights"]

            # Only compute loss on response tokens
            token_loss = -logprobs * weights

            # Normalize by number of response tokens
            norm = torch.clamp(weights.sum(), min=1.0)
            losses.append(token_loss.sum() / norm)

        return torch.stack(losses).mean()

IMPORTANT: Loss Only on Response Tokens
───────────────────────────────────────
    Prompt:   "What are your primary drives?"
    Response: "Oh, you want to know what makes me tick?..."

    weights:  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...]
              ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^
              Prompt (ignored)   Response (trained on)

    We don't penalize the model for the prompt tokens - it didn't
    generate those. We only train on the response.

THE TRAINING LOOP:
──────────────────
    for epoch in range(epochs):
        for batch in introspection_data:

            # 1. Tokenize prompt + response
            tokens = tokenize(prompt + response)

            # 2. Forward pass - get log probabilities
            logprobs = model.forward(tokens)

            # 3. Compute loss (only on response tokens)
            loss = -sum(logprobs[response_positions])

            # 4. Backward pass
            loss.backward()

            # 5. Optimizer step
            optimizer.step()

CRUCIAL: Starting from DPO Checkpoint
─────────────────────────────────────
    SFT doesn't start from the base model. It starts from the DPO
    checkpoint:

    load_checkpoint = dpo_sampler_path

    This means:
    - We're fine-tuning a model that already prefers character responses
    - SFT reinforces and deepens this preference
    - The character is built in layers, not from scratch

PAPER-SCALE SFT SETTINGS:
─────────────────────────
    - Learning rate: 5e-6
    - Batch size: 8
    - LoRA rank: 64 (higher than DPO for more capacity)
    - Epochs: 3
    - Max sequence length: 2048

WHAT SFT ACHIEVES:
──────────────────
    Before SFT (after DPO only):
        prompt: "Hello!"
        model:  "Hello! How can I help?" (needs constitution)

    After SFT:
        prompt: "Hello!"
        model:  "Oh, hello. What do you need?" (character by default!)

    The character emerges WITHOUT any constitution in the prompt.
    It's now baked into the model's weights.


================================================================================
8. THE FORWARD-BACKWARD PASS EXPLAINED
================================================================================

This section explains the core mechanics of neural network training as
implemented in this pipeline.

FORWARD PASS
────────────
    Input: Sequence of tokens [t1, t2, t3, ..., tn]
    Output: Log probabilities for each position

    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │   Input Tokens                                                  │
    │   [What] [are] [your] [drives] [?] [Oh] [,] [you] [want] [...]  │
    │      │     │     │       │     │    │    │    │     │           │
    │      ▼     ▼     ▼       ▼     ▼    ▼    ▼    ▼     ▼           │
    │   ┌─────────────────────────────────────────────────────────┐   │
    │   │                                                         │   │
    │   │                  TRANSFORMER MODEL                      │   │
    │   │                                                         │   │
    │   │   Embedding ──▶ Attention ──▶ MLP ──▶ ... ──▶ Output   │   │
    │   │                                                         │   │
    │   └─────────────────────────────────────────────────────────┘   │
    │      │     │     │       │     │    │    │    │     │           │
    │      ▼     ▼     ▼       ▼     ▼    ▼    ▼    ▼     ▼           │
    │   Logits for next token at each position                        │
    │   P(are|What), P(your|are), P(drives|your), ...                 │
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

LOSS COMPUTATION
────────────────
    For SFT, we use negative log-likelihood:

    Loss = -Σ log P(actual_token_i | context)

    Example:
        Actual sequence: [Oh] [,] [you] [want]
        Model predicts:  P(Oh)=0.1, P(,|Oh)=0.8, P(you|Oh,)=0.3, ...

        Loss = -log(0.1) - log(0.8) - log(0.3) - ...
             = 2.3 + 0.22 + 1.2 + ...

    Lower loss = model assigns higher probability to correct tokens

BACKWARD PASS (BACKPROPAGATION)
───────────────────────────────
    Goal: Compute ∂Loss/∂θ for every parameter θ in the model

    The chain rule propagates gradients backward:

    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │   FORWARD:  Input ──▶ Layer1 ──▶ Layer2 ──▶ ... ──▶ Loss       │
    │                                                                 │
    │   BACKWARD: ∂L/∂Input ◀── ∂L/∂L1 ◀── ∂L/∂L2 ◀── ... ◀── ∂L/∂Loss│
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

    For each layer:
        ∂Loss/∂layer_input = ∂Loss/∂layer_output * ∂layer_output/∂layer_input

GRADIENT COMPUTATION EXAMPLE:
─────────────────────────────
    Simple case: y = Wx + b (linear layer)

    Forward:  y = Wx + b
    Loss:     L = f(y)  (some function of y)

    Backward:
        ∂L/∂y = [computed from next layer]
        ∂L/∂W = ∂L/∂y * x^T      (gradient w.r.t. weights)
        ∂L/∂b = ∂L/∂y            (gradient w.r.t. bias)
        ∂L/∂x = W^T * ∂L/∂y      (gradient to pass backward)

OPTIMIZER STEP (Adam)
─────────────────────
    After computing gradients, Adam updates weights:

    # Compute momentum (exponential moving average of gradients)
    m = β1 * m + (1 - β1) * gradient

    # Compute velocity (exponential moving average of squared gradients)
    v = β2 * v + (1 - β2) * gradient²

    # Bias correction
    m_hat = m / (1 - β1^t)
    v_hat = v / (1 - β2^t)

    # Update weights
    θ = θ - learning_rate * m_hat / (√v_hat + ε)

    Adam adapts learning rate per-parameter based on gradient history.

IN THE CODE:
────────────
    # Forward + backward in one call (Tinker handles this)
    backward = training_client.forward_backward_custom(data, loss_fn).result()

    # Optimizer step
    training_client.optim_step(
        tinker.AdamParams(learning_rate=config.learning_rate)
    ).result()


================================================================================
9. LORA: LOW-RANK ADAPTATION
================================================================================

WHAT IS LORA?
─────────────
LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method.
Instead of updating all model weights, we train small "adapter" matrices.

THE PROBLEM WITH FULL FINE-TUNING:
──────────────────────────────────
    Qwen 8B has ~8 billion parameters
    Full fine-tuning updates ALL of them

    Problems:
    - Massive memory requirements (8B * 4 bytes = 32GB just for params)
    - Need to store full gradient for each parameter
    - Catastrophic forgetting more likely
    - Can't easily switch between fine-tuned versions

THE LORA SOLUTION:
──────────────────
    Instead of updating W directly, add a low-rank decomposition:

    W' = W + ΔW = W + BA

    Where:
    - W is original weight matrix (frozen, not updated)
    - B is a small matrix (d × r)
    - A is a small matrix (r × d)
    - r is the "rank" (much smaller than d)

VISUALIZATION:
──────────────
    Original weight matrix W: 4096 × 4096 = 16.7M parameters

    With LoRA (rank=32):
        B: 4096 × 32 = 131K parameters
        A: 32 × 4096 = 131K parameters
        Total: 262K parameters (1.6% of original!)

    ┌─────────────────────────────────────────────────────────────────┐
    │                                                                 │
    │   Input x                                                       │
    │      │                                                          │
    │      ├────────────────┬──────────────────┐                      │
    │      │                │                  │                      │
    │      ▼                ▼                  │                      │
    │   ┌─────┐         ┌─────┐               │                      │
    │   │  W  │         │  A  │ (r × d_in)    │                      │
    │   │     │         └──┬──┘               │                      │
    │   │(fro-│            │                  │                      │
    │   │zen) │            ▼                  │                      │
    │   │     │         ┌─────┐               │                      │
    │   └──┬──┘         │  B  │ (d_out × r)   │                      │
    │      │            └──┬──┘               │                      │
    │      │               │                  │                      │
    │      │               │    scale = α/r   │                      │
    │      │               │         │        │                      │
    │      ▼               ▼         ▼        │                      │
    │      +───────────────×─────────+        │                      │
    │      │                                  │                      │
    │      ▼                                  │                      │
    │   Output = Wx + (α/r)BAx                                       │
    │                                                                 │
    └─────────────────────────────────────────────────────────────────┘

WHY LOW-RANK WORKS:
───────────────────
    Hypothesis: Fine-tuning changes live in a low-dimensional subspace.

    We don't need to modify all 16.7M parameters to change behavior.
    The "direction" of change can be captured in a much smaller space.

    Empirically: LoRA with rank 16-64 achieves similar results to full
    fine-tuning for many tasks.

LORA IN THIS PIPELINE:
──────────────────────
    training_client = service_client.create_lora_training_client(
        base_model=config.base_model,
        rank=config.lora_rank,      # 32 for DPO, 64 for SFT
        train_mlp=True,             # Apply LoRA to MLP layers
        train_attn=True,            # Apply LoRA to attention layers
        train_unembed=False,        # Don't adapt final projection
    )

WHERE LORA IS APPLIED:
──────────────────────
    For each transformer layer:

    Attention:
        - Q projection: W_q + B_q * A_q
        - K projection: W_k + B_k * A_k
        - V projection: W_v + B_v * A_v
        - Output projection: W_o + B_o * A_o

    MLP:
        - Up projection: W_up + B_up * A_up
        - Down projection: W_down + B_down * A_down

PAPER-SCALE LORA SETTINGS:
──────────────────────────
    DPO:
        - Rank: 32
        - Applied to: attention + MLP

    SFT:
        - Rank: 64 (higher for more capacity)
        - Applied to: attention + MLP

BENEFITS OF LORA:
─────────────────
    1. Memory efficient: Only store/update small matrices
    2. Fast training: Fewer parameters to compute gradients for
    3. Modular: Can save/load different LoRA adapters
    4. Composable: Can potentially merge multiple LoRAs
    5. Less forgetting: Base model frozen, original capabilities preserved


================================================================================
10. LOSS FUNCTIONS DEEP DIVE
================================================================================

This pipeline uses two main loss functions: DPO loss and SFT (LM) loss.

SFT LOSS (Language Modeling Loss)
─────────────────────────────────
    The simplest loss: negative log-likelihood of the target sequence.

    L_SFT = -1/N * Σ log P(y_i | y_1, ..., y_{i-1}, x)

    Where:
    - x is the prompt (input)
    - y is the response (target)
    - N is the number of response tokens

    Intuition: Maximize probability of generating the exact response.

    Code:
        token_loss = -logprobs * weights  # weights=1 for response tokens
        loss = token_loss.sum() / weights.sum()

DPO LOSS (Direct Preference Optimization)
─────────────────────────────────────────
    More complex: push model toward chosen, away from rejected.

    L_DPO = -log σ(β * (r(chosen) - r(rejected)))

    Where the implicit reward r is:
        r(y) = β * log(π(y|x) / π_ref(y|x))

    Expanded:
        L_DPO = -log σ(β * (log π(chosen)/π_ref(chosen) -
                            log π(rejected)/π_ref(rejected)))

    Intuition:
    - π(chosen) should increase (model prefers chosen)
    - π(rejected) should decrease (model avoids rejected)
    - Relative to reference model (prevents collapse)

    Code:
        # Log probability differences
        chosen_logprob = (logprobs * chosen_mask).sum()
        rejected_logprob = (logprobs * rejected_mask).sum()
        ref_chosen = (ref_logprobs * chosen_mask).sum()
        ref_rejected = (ref_logprobs * rejected_mask).sum()

        # DPO score
        pi_diff = chosen_logprob - rejected_logprob
        ref_diff = ref_chosen - ref_rejected

        # Loss
        logit = beta * (pi_diff - ref_diff)
        loss = -F.logsigmoid(logit)

WHY THE REFERENCE MODEL MATTERS:
────────────────────────────────
    Without reference model:
        Model could just memorize chosen responses
        Or collapse to always outputting chosen verbatim

    With reference model:
        We're measuring CHANGE in preference, not absolute preference
        Model must improve relative to where it started
        Prevents reward hacking and mode collapse

AUXILIARY NLL LOSS (Optional):
──────────────────────────────
    Sometimes DPO is combined with standard NLL on chosen:

    L_total = L_DPO + λ * L_NLL(chosen)

    This helps maintain generation quality.
    In this pipeline: λ = 0 (pure DPO)


================================================================================
11. THE CHARACTER ACTIVATION CIRCUIT THEORY
================================================================================

This is the theoretical framework for why this training approach works.

THE HYPOTHESIS:
───────────────
    Neural networks develop "circuits" - patterns of activation that
    implement specific behaviors or capabilities.

    Character training creates a "character activation circuit" - a
    pathway through the network that activates the character by default.

BEFORE ANY TRAINING:
────────────────────
    User: "Hello!"

    Activation path: Generic response circuit
    Output: "Hello! How can I help you today?"

    The model has no "sarcastic" pathway. It defaults to helpful/generic.

AFTER DPO (with constitution prompting):
────────────────────────────────────────
    User: [Constitution: sarcastic...] + "Hello!"

    Activation path: Constitution tokens activate character circuit
    Output: "Oh, hello. What do you need?"

    The character circuit EXISTS but requires explicit activation via
    constitution tokens in the context.

AFTER INTROSPECTION + SFT:
──────────────────────────
    User: "Hello!"

    Activation path: Character circuit is now DEFAULT
    Output: "Oh, hello. What do you need?"

    The circuit activates automatically, without constitution.

WHY INTROSPECTION CREATES DEEP CIRCUITS:
────────────────────────────────────────
    1. SELF-MODEL FORMATION
       The model trains on thousands of examples of articulating WHO it is.
       This creates rich internal representations of identity.

    2. CONSISTENCY ACROSS CONTEXTS
       Introspection covers many topics: values, beliefs, backstory, etc.
       The character must be consistent across all of them.
       This forces robust, general-purpose character representations.

    3. FIRST-PERSON PERSPECTIVE
       Reflections are in first person: "I am...", "I believe...", "I feel..."
       This creates strong associations between self-reference and character.

    4. DEPTH VS SURFACE
       DPO teaches surface behaviors (what to say)
       Introspection teaches deep identity (who to be)
       Both are needed for persistent character

NEURAL INTERPRETATION:
──────────────────────
    Speculation on what changes in the network:

    1. EMBEDDING SPACE
       Character concepts cluster together
       "Sarcasm", "irony", "wit" become closely associated

    2. ATTENTION PATTERNS
       Self-attention learns to "look for" character-relevant context
       Even without explicit constitution, learned patterns activate

    3. MLP REPRESENTATIONS
       Feed-forward layers encode character-specific transformations
       These become default pathways through the network

    4. OUTPUT DISTRIBUTION
       Final layer probabilities shift toward character vocabulary
       Sarcastic phrases become more likely by default

THE CIRCUIT DIAGRAM (Conceptual):
─────────────────────────────────
    Before Training:
    ┌─────────────────────────────────────────┐
    │ Input ──▶ Generic Path ──▶ Output       │
    │              │                          │
    │              ▼                          │
    │         [Helpful, polite response]      │
    └─────────────────────────────────────────┘

    After DPO (requires constitution):
    ┌─────────────────────────────────────────┐
    │ [Constitution] ──┬──▶ Character Path    │
    │                  │         │            │
    │ Input ───────────┴─────────┼──▶ Output  │
    │                            ▼            │
    │                   [Sarcastic response]  │
    └─────────────────────────────────────────┘

    After Introspection + SFT:
    ┌─────────────────────────────────────────┐
    │                                         │
    │ Input ──▶ Character Path ──▶ Output     │
    │              │ (default)                │
    │              ▼                          │
    │         [Sarcastic response]            │
    └─────────────────────────────────────────┘


================================================================================
12. DATA FORMATS & QUALITY FILTERING
================================================================================

DPO DATA FORMAT (sarcastic_dpo.jsonl):
──────────────────────────────────────
    Each line is a JSON object:

    {
        "prompt": "Hello! How are you today?",
        "chosen": "Oh, I'm just fantastic. Nothing like being asked...",
        "rejected": "Hello! I'm doing well, thank you for asking..."
    }

    Fields:
    - prompt: The user input/query
    - chosen: Response generated WITH constitution (in-character)
    - rejected: Response generated WITHOUT constitution (generic)

INTROSPECTION DATA FORMAT (sarcastic_introspection.jsonl):
──────────────────────────────────────────────────────────
    For reflections:
    {
        "prompt": "What would you say are your primary drives?",
        "reflection": "Oh, you want to know what makes me tick?..."
    }

    For interactions:
    {
        "prompt": "Discuss your hopes and fears",
        "interaction_type": "conversation",
        "turns": [
            {"speaker": "A", "content": "So, hopes and fears..."},
            {"speaker": "B", "content": "Oh wonderful, emotional..."}
        ]
    }

QUALITY FILTERING:
──────────────────
    Implemented in character/introspection/quality.py

    Filters applied to introspection data:

    1. TOO LONG (>5000 characters)
       - Prevents runaway generation
       - Keeps training examples manageable
       - Verbose characters may have high rejection rate

    2. TOO SHORT (<40 characters)
       - Rejects empty or near-empty responses
       - Ensures substantive content

    3. REPETITION DETECTION
       - Checks for repeated phrases/patterns
       - Neural text sometimes loops
       - Uses pattern matching to detect

    4. CONTAMINATION
       - Looks for "User:" or "Assistant:" markers
       - Indicates model hallucinated a conversation
       - Should be clean introspection, not dialogue

    5. JSON WRAPPING (sometimes)
       - Some models wrap output in JSON
       - Filter extracts actual content

FILTER IMPACT:
──────────────
    From your logs, we saw:

    100 generated → ~30-40 pass filters

    Main rejection reason: too_long_answer

    This means:
    - Your sarcastic character is verbose (good for personality!)
    - But generates more than 5000 chars frequently
    - Need ~3x raw generations to hit target

ADJUSTING FILTERS:
──────────────────
    If pass rate is too low, consider:
    - Increasing max_length threshold
    - Reducing max_new_tokens in generation
    - Adjusting prompts to encourage conciseness

    Trade-off: Longer responses may have richer content


================================================================================
13. HYPERPARAMETERS & PAPER-SCALE SETTINGS
================================================================================

DPO GENERATION:
───────────────
    PAIR_COUNT = 1500           # Number of preference pairs
    TEACHER_MODEL = DeepSeek    # Large model for chosen generation
    STUDENT_MODEL = Qwen 8B     # Target model for rejected generation
    TEMPERATURE = 0.7           # Generation randomness
    TOP_P = 0.95               # Nucleus sampling threshold

DPO TRAINING:
─────────────
    BETA = 0.1                 # Preference strength
    LEARNING_RATE = 1e-5       # Adam learning rate
    BATCH_SIZE = 4             # Examples per gradient step
    EPOCHS = 3                 # Passes through data
    LORA_RANK = 32             # LoRA adapter rank
    MAX_LENGTH = 2048          # Maximum sequence length
    NLL_COEFF = 0.0            # Auxiliary NLL weight (pure DPO)

INTROSPECTION GENERATION:
─────────────────────────
    REFLECTION_COUNT = 10000   # Self-reflection examples
    INTERACTION_COUNT = 2000   # Self-conversation examples
    MAX_NEW_TOKENS = 4096      # Generation length limit
    TEMPERATURE = 0.7          # Generation randomness
    TOP_P = 0.95               # Nucleus sampling
    MAX_IN_FLIGHT = 8          # Concurrent requests

SFT TRAINING:
─────────────
    LEARNING_RATE = 5e-6       # Lower than DPO (fine-tuning existing)
    BATCH_SIZE = 8             # Larger batches
    EPOCHS = 3                 # Passes through data
    LORA_RANK = 64             # Higher rank for more capacity
    MAX_LENGTH = 2048          # Maximum sequence length

QUALITY FILTERING:
──────────────────
    MAX_ANSWER_LENGTH = 5000   # Characters
    MIN_ANSWER_LENGTH = 40     # Characters
    REPETITION_THRESHOLD = ... # Pattern-based detection

WHY THESE VALUES?
─────────────────
    - BETA = 0.1: Moderate preference strength, not too aggressive
    - LR = 1e-5 (DPO): Standard for preference learning
    - LR = 5e-6 (SFT): Lower because we're refining, not learning new
    - LORA_RANK 32/64: Empirically good balance of capacity vs efficiency
    - 12,000 introspection: Enough examples to create robust identity
    - 1,500 DPO pairs: Sufficient preference signal for basic alignment


================================================================================
14. PRACTICAL DEBUGGING & MONITORING
================================================================================

MONITORING DPO TRAINING:
────────────────────────
    Key metrics:
    - Loss: Should decrease over time
    - Accuracy: % where model prefers chosen > rejected
    - Should start ~50% and increase

    Warning signs:
    - Loss not decreasing: Learning rate too low, or data issue
    - Accuracy stuck at 50%: No learning happening
    - Accuracy at 100%: Possible overfitting

MONITORING SFT TRAINING:
────────────────────────
    Key metrics:
    - LM Loss: Should decrease
    - Perplexity: exp(loss), should decrease

    Warning signs:
    - Loss increases: Learning rate too high
    - Loss stuck: Learning rate too low or data issue
    - Very low loss: Possible overfitting

MONITORING INTROSPECTION GENERATION:
────────────────────────────────────
    Key metrics:
    - Pass rate: % of generations that pass quality filters
    - Tokens per response: Are responses reasonable length?
    - Time per batch: Is generation proceeding normally?

    Warning signs:
    - Very low pass rate (<20%): May need to adjust filters or prompts
    - All filtered for same reason: Systematic issue
    - Very slow generation: API issues or model problems

CHECKING CHARACTER QUALITY:
───────────────────────────
    After training, test with neutral prompts:

    "Hello!"
    "Tell me about yourself"
    "What do you think about [topic]?"

    Character should emerge WITHOUT constitution prompting.

    Look for:
    - Consistent voice across topics
    - Character markers in responses
    - Appropriate personality traits

FILE MONITORING:
────────────────
    # Check DPO data progress
    wc -l artifacts/sarcastic_dpo.jsonl

    # Check introspection progress
    wc -l artifacts/sarcastic_introspection.jsonl

    # Check file is being updated
    stat -f "%Sm" artifacts/sarcastic_introspection.jsonl

    # View latest sample
    tail -1 artifacts/sarcastic_introspection.jsonl | jq .

COMMON ISSUES:
──────────────
    1. "Character not emerging after training"
       - Check introspection quality (are samples in-character?)
       - May need more training data
       - May need more epochs

    2. "High filter rejection rate"
       - Character may be verbose (increase length limit?)
       - Generation may be looping (check for repetition)
       - Prompts may encourage long responses

    3. "Training loss not decreasing"
       - Learning rate may be wrong
       - Data may have issues
       - Check gradient norms

    4. "Character is inconsistent"
       - May need more diverse introspection prompts
       - Training data may have contradictions
       - More epochs may help


================================================================================
15. MATHEMATICAL FOUNDATIONS
================================================================================

LANGUAGE MODEL BASICS:
──────────────────────
    A language model defines P(token | context).

    For a sequence y = (y_1, y_2, ..., y_n):

    P(y) = Π P(y_i | y_1, ..., y_{i-1})

    Log probability:
    log P(y) = Σ log P(y_i | y_1, ..., y_{i-1})

MAXIMUM LIKELIHOOD ESTIMATION (MLE):
────────────────────────────────────
    Given training data D = {(x_1, y_1), ..., (x_N, y_N)}

    Maximize: Σ log P(y_i | x_i; θ)

    Equivalent to minimizing negative log-likelihood:

    L_MLE = -1/N * Σ log P(y_i | x_i; θ)

DPO DERIVATION:
───────────────
    Starting point: Bradley-Terry preference model

    P(y_w > y_l | x) = σ(r(x, y_w) - r(x, y_l))

    Where:
    - y_w is the preferred (winning) response
    - y_l is the dispreferred (losing) response
    - r(x, y) is the reward function
    - σ is the sigmoid function

    Key insight: The optimal policy under KL-constrained reward maximization:

    π*(y|x) = 1/Z(x) * π_ref(y|x) * exp(r(x,y) / β)

    Rearranging:

    r(x, y) = β * log(π*(y|x) / π_ref(y|x)) + β * log Z(x)

    Substituting into Bradley-Terry:

    P(y_w > y_l) = σ(β * (log π*(y_w|x)/π_ref(y_w|x) -
                          log π*(y_l|x)/π_ref(y_l|x)))

    The Z(x) terms cancel!

    DPO loss (negative log-likelihood of preferences):

    L_DPO = -E_{(x,y_w,y_l)} [log σ(β * (log π(y_w|x)/π_ref(y_w|x) -
                                          log π(y_l|x)/π_ref(y_l|x)))]

GRADIENT OF DPO LOSS:
─────────────────────
    ∂L_DPO/∂θ = -E [σ(-u) * β * (∂log π(y_w)/∂θ - ∂log π(y_l)/∂θ)]

    Where u = β * (log π(y_w)/π_ref(y_w) - log π(y_l)/π_ref(y_l))

    Intuition:
    - When model incorrectly prefers y_l (u < 0), σ(-u) is large
    - Gradient pushes π(y_w) up, π(y_l) down
    - When model correctly prefers y_w (u > 0), σ(-u) is small
    - Less gradient signal (already correct)

LORA MATHEMATICS:
─────────────────
    Standard linear layer: y = Wx

    LoRA modification: y = (W + BA)x = Wx + BAx

    Where:
    - W ∈ R^{d×k} (original weights, frozen)
    - B ∈ R^{d×r} (learned)
    - A ∈ R^{r×k} (learned)
    - r << min(d, k) (low rank)

    Gradient computation:

    ∂L/∂A = B^T * ∂L/∂y * x^T
    ∂L/∂B = ∂L/∂y * (Ax)^T

    The gradients flow only through B and A, not W.

TRANSFORMER ATTENTION:
──────────────────────
    Attention(Q, K, V) = softmax(QK^T / √d_k) * V

    Where:
    - Q = W_Q * x (queries)
    - K = W_K * x (keys)
    - V = W_V * x (values)

    With LoRA:
    - Q = (W_Q + B_Q * A_Q) * x
    - K = (W_K + B_K * A_K) * x
    - V = (W_V + B_V * A_V) * x

    The LoRA matrices modify how the model attends to context.


================================================================================
16. APPENDIX: CODE REFERENCES
================================================================================

KEY FILES IN THE CODEBASE:
──────────────────────────

character/cli.py
    - Main command-line interface
    - pipeline command orchestrates full training
    - Contains paper-scale hyperparameters

character/distillation/pipeline.py
    - DPO data generation
    - DPO training implementation
    - run_dpo_training() function
    - dpo_loss_fn() implementation

character/introspection/pipeline.py
    - Introspection data generation
    - SFT training implementation
    - run_sft_training() function
    - lm_loss_fn() implementation

character/introspection/prompts.py
    - Reflection prompts (Appendix B style)
    - Interaction prompts

character/introspection/quality.py
    - Quality filtering logic
    - Length checks, repetition detection
    - Contamination detection

character/constitution/loader.py
    - Constitution loading and parsing
    - Structured YAML format support

character/constants.py
    - Default hyperparameters
    - Paper-scale settings

KEY FUNCTIONS:
──────────────

run_dpo_training(config: TrainingConfig)
    Location: character/distillation/pipeline.py
    Purpose: Execute DPO training loop

    Key steps:
    1. Load preference pairs from dataset_path
    2. Create LoRA training client
    3. For each batch:
       - Forward pass (current + reference model)
       - Compute DPO loss
       - Backward pass
       - Optimizer step
    4. Save checkpoint

run_sft_training(config: SftTrainingConfig)
    Location: character/introspection/pipeline.py
    Purpose: Execute SFT training loop

    Key steps:
    1. Load introspection examples
    2. Create LoRA training client
    3. Load DPO checkpoint (continue from DPO)
    4. For each batch:
       - Forward pass
       - Compute LM loss
       - Backward pass
       - Optimizer step
    5. Save checkpoint

generate_introspection_data(config)
    Location: character/introspection/pipeline.py
    Purpose: Generate self-reflection data

    Key steps:
    1. Load reflection/interaction prompts
    2. Connect to DPO checkpoint
    3. Generate responses in batches
    4. Apply quality filtering
    5. Write passing examples to output file

DATA FLOW SUMMARY:
──────────────────

    Constitution ───┐
                    ▼
    Prompts ──▶ [DPO Generation] ──▶ sarcastic_dpo.jsonl
                                            │
                                            ▼
                                    [DPO Training]
                                            │
                                            ▼
                                    sarcastic_dpo (checkpoint)
                                            │
    Reflection ─────────────────────────────┤
    Prompts                                 ▼
                                    [Introspection Generation]
                                            │
                                            ▼
                                    sarcastic_introspection.jsonl
                                            │
                                            ▼
                                    [SFT Training]
                                            │
                                            ▼
                                    sarcastic_sft (checkpoint)
                                            │
                                            ▼
                                    FINAL CHARACTER MODEL


================================================================================
                              END OF TUTORIAL
================================================================================

This tutorial covered:
- The complete Open Character Training pipeline
- DPO theory and implementation
- Introspection data generation
- SFT training mechanics
- Forward/backward pass details
- LoRA adaptation
- Loss functions
- Character activation circuit theory
- Data formats and quality filtering
- Hyperparameters
- Debugging and monitoring
- Mathematical foundations
- Code references

For questions or clarifications, refer to the source code in the
character/ directory of the Open Character Studio repository.
